loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(),d.train),
learn_rate(),
penalty(),
size = 30
) %>%
mutate(mtry = mtry/length(d.train))
library(rules)
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry       = tune(),
learn_rate = tune(),
penalty    = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(),d.train),
learn_rate(),
penalty(),
size = 30
) %>%
# Converting mtry into ratio, as xrf uses ratios instead of nominal colsample_bytree
mutate(mtry = mtry/length(d.train))
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
best_xrf <- tune_xrf %>%
select_best(metric = "accuracy")
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(),d.train),
learn_rate(),
penalty(),
size = 30
) %>%
# Converting mtry into ratio, as xrf uses ratios instead of nominal colsample_bytree
mutate(mtry = mtry/length(d.train))
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
tune_xrf$.notes[[1]] %>% as.character()
tune_xrf$.notes %>% as.character()
tune_xrf$.notes
tune_xrf
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry       = tune(),
learn_rate = tune(),
penalty    = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
tune_xrf
tune_xrf$.notes
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
penalty(),
size = 30
)
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
best_xrf <- tune_xrf %>%
select_best(metric = "accuracy")
tune_xrf$.notes
rule_fit()
rule_fit
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
penalty(),
size = 30
)
grid_xrf
?xrf
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune()
) %>%
set_engine("rules") %>%
set_mode("classification")
show_engines('rule_fit')
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
penalty(),
size = 30
)
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune(),
mtry       = 0.5
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
penalty(),
size = 30
)
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
tune_xrf$.notes
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune(),
mtry       = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
penalty(),
mtry_prop(range = c(0.1, 1), trans = NULL),
size = 30
)
grid_xrf
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune(),
mtry_prop  = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
model_xrf <-
rule_fit(
trees      = 1000,
tree_depth = tune(),
min_n      = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
penalty    = tune(),
mtry       = tune()
) %>%
set_engine("xrf") %>%
set_mode("classification")
wf_xrf <- workflow() %>%
add_model(model_xrf) %>%
add_recipe(recipe)
grid_xrf <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
penalty(),
mtry = mtry_prop(range = c(0.1, 1), trans = NULL),
size = 30
)
set.seed(8675309)
tune_xrf <- wf_xrf %>%
tune_grid(resamples = cv.fold,
grid = grid_xrf,
control = control_grid(save_pred = T),
metrics = metric_set(accuracy))
tune_xrf$.notes
model_lsvm <-
svm_linear(
cost = tune()
) %>%
set_engine("LiblineaR") %>%
set_mode("classification")
grid_regular(
cost(),
size = 30
)
recipe <- recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors())
recipe %>% prep() %>% bake(d.train)
recipe %>% prep() %>% bake(d.train)
recipe <- recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors())
recipe
recipe
recipe <- recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors())
recipe
set.seed(8675309)
splits <- initial_split(df,
prop = .8,
strata = y)
d.train <- training(splits)
d.test  <- testing(splits)
cv.fold <- vfold_cv(d.train, v = 4)
recipe <- recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors())
recipe %>% prep() %>% bake(d.train)
recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>% prep() %>% bake(d.train)
df <- dat_no_na
df <- df %>%
mutate(
race = race %>% factor(),
family_history = family_history %>% factor(),
first_degree_history = first_degree_history %>% factor(),
# Shortening groups
tea = case_when(tea == 0 ~ "0.wk",
0 < tea & tea <= 2 ~ "2fewer.wk",
2 < tea & tea <= 7 ~ "7fewer.wk",
7 < tea ~ "7more.wk")
) %>%
mutate(y = y %>% factor())
set.seed(8675309)
splits <- initial_split(df,
prop = .8,
strata = y)
d.train <- training(splits)
d.test  <- testing(splits)
cv.fold <- vfold_cv(d.train, v = 4)
recipe <- recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>% prep() %>% bake(d.train)
gc()
setwd("G:/My Drive/academics/2122_uic_ms/2022_spring/ids506_health/prostate")
library(tidyverse)
library(tidymodels)
library(patchwork)
library(lubridate)
library(tidytext)
library(themis)
dat <- read_csv("data/training_data.csv") %>%
rename(y = survival_7_years)
dat <- dat %>%
filter(survival_1_year == 1)
dat_no_na <- dat %>%
filter(
!is.na(weight),
!is.na(height),
!is.na(age),
!is.na(gleason_score),
!is.na(tumor_diagnosis),
!is.na(race)
) %>%
mutate(
tumor_6_months       = ifelse(is.na(tumor_6_months)==1,0,tumor_6_months),
psa_6_months         = ifelse(is.na(psa_6_months)==1,0,psa_6_months),
psa_1_year           = ifelse(is.na(psa_1_year)==1,0,psa_1_year),
tumor_1_year         = ifelse(is.na(tumor_1_year)==1,0,tumor_1_year),
family_history       = ifelse(is.na(family_history)==1,0,family_history),
first_degree_history = ifelse(is.na(first_degree_history)==1,0,first_degree_history),
previous_cancer      = ifelse(is.na(previous_cancer)==1,0,previous_cancer),
smoker               = ifelse(is.na(smoker)==1,0,smoker),
tea                  = ifelse(is.na(tea)==1,0,tea),
psa_diagnosis        = ifelse(is.na(psa_diagnosis)==1,0,psa_diagnosis)
) %>%
select(-id)
dat_no_na <- dat_no_na %>%
mutate(
# Replacing 00 date values with 01
zero_day = ifelse(str_detect(diagnosis_date,"00"),1,0),
diagnosis_date = ifelse(zero_day == 0,
diagnosis_date,
str_replace(diagnosis_date, "00","01")),
diagnosis_date = paste(diagnosis_date,"2021",sep="") %>% mdy() %>% yday()
) %>%
select(-zero_day)
df <- dat_no_na
df <- df %>%
mutate(
race = race %>% factor(),
family_history = family_history %>% factor(),
first_degree_history = first_degree_history %>% factor(),
# Shortening groups
tea = case_when(tea == 0 ~ "0.wk",
0 < tea & tea <= 2 ~ "2fewer.wk",
2 < tea & tea <= 7 ~ "7fewer.wk",
7 < tea ~ "7more.wk")
) %>%
mutate(y = y %>% factor())
set.seed(8675309)
splits <- initial_split(df,
prop = .8,
strata = y)
d.train <- training(splits)
d.test  <- testing(splits)
cv.fold <- vfold_cv(d.train, v = 4)
recipe(y~., data = d.train) %>%
step_relevel(side,    ref_level = "left") %>%
step_relevel(t_score, ref_level = "T1a") %>%
step_relevel(n_score, ref_level = "N0") %>%
step_relevel(m_score, ref_level = "M0") %>%
step_relevel(stage,   ref_level = "I") %>%
step_relevel(race,    ref_level = "4") %>%
step_relevel(family_history, ref_level = "0") %>%
step_relevel(first_degree_history, ref_level = "0") %>%
step_relevel(tea,     ref_level = "0.wk") %>%
step_tokenize(symptoms,options = list(strip_punct = T,
lowercase = T)) %>%
step_tf(symptoms) %>%
step_dummy(t_score,n_score,m_score,stage,race,family_history,first_degree_history,side,tea) %>%
step_zv(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>% prep() %>% bake(d.train)
d.train
